{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Backpropication Matrix form from Michael A. Nielson\n",
    "Using the matrix approach to back-propagation with mini-batches, and how it can fasten the learning of a neural network, using this approach with looping over a mini-batch and see how the mini-batch size can affect the learning’s speed and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this neural network taking training data of the MNIST, it has 784 input neuron, 10 output neurons, and a hidden layer of 30 neurons. By setting network to run over 30 epochs, with a learning rate of η=3.0 and a mini-batch size of 1 or 10, we get the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matrix apporach\n",
    "update min batch size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "\n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test));\n",
    "            else:\n",
    "                print(\"Epoch {} complete\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] \n",
    "        zs = [] \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 8027 / 10000\n",
      "Epoch 1 : 8235 / 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sonali\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:83: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 : 7503 / 10000\n",
      "Epoch 3 : 7753 / 10000\n",
      "Epoch 4 : 8236 / 10000\n",
      "Epoch 5 : 8626 / 10000\n",
      "Epoch 6 : 8594 / 10000\n",
      "Epoch 7 : 8633 / 10000\n",
      "Epoch 8 : 8495 / 10000\n",
      "Epoch 9 : 8369 / 10000\n",
      "Epoch 10 : 8696 / 10000\n",
      "Epoch 11 : 8597 / 10000\n",
      "Epoch 12 : 8667 / 10000\n",
      "Epoch 13 : 8647 / 10000\n",
      "Epoch 14 : 8647 / 10000\n",
      "Epoch 15 : 8499 / 10000\n",
      "Epoch 16 : 8745 / 10000\n",
      "Epoch 17 : 8844 / 10000\n",
      "Epoch 18 : 8947 / 10000\n",
      "Epoch 19 : 8758 / 10000\n",
      "Epoch 20 : 8822 / 10000\n",
      "Epoch 21 : 8895 / 10000\n",
      "Epoch 22 : 8919 / 10000\n",
      "Epoch 23 : 8869 / 10000\n",
      "Epoch 24 : 8788 / 10000\n",
      "Epoch 25 : 9018 / 10000\n",
      "Epoch 26 : 9029 / 10000\n",
      "Epoch 27 : 8852 / 10000\n",
      "Epoch 28 : 8928 / 10000\n",
      "Epoch 29 : 8904 / 10000\n",
      "--- 308.43776631355286 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "training_data, validation_data, test_data = load_data_wrapper()\n",
    "net = Network([784, 30, 10])\n",
    "net.SGD(training_data, 30, 1, 3.0, test_data=test_data)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " it took almost 5 mins to run with min batch size =1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matrix apporach \n",
    "update min batch size =30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkMatrix(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "     \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def feedforward2(self, a):\n",
    "        zs = []\n",
    "        activations = [a]\n",
    "\n",
    "        activation = a\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "\n",
    "        return (zs, activations)\n",
    "        \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "\n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test));\n",
    "            else:\n",
    "                print(\"Epoch {} complete\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        batch_size = len(mini_batch)\n",
    "\n",
    "        \n",
    "        x = np.asarray([_x.ravel() for _x, _y in mini_batch]).transpose()\n",
    "        y = np.asarray([_y.ravel() for _x, _y in mini_batch]).transpose()\n",
    "\n",
    "        nabla_b, nabla_w = self.backprop(x, y)\n",
    "        self.weights = [w - (eta / batch_size) * nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (eta / batch_size) * nb for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "        return\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "\n",
    "        nabla_b = [0 for i in self.biases]\n",
    "        nabla_w = [0 for i in self.weights]\n",
    "\n",
    "        # feedforward\n",
    "        zs, activations = self.feedforward2(x)\n",
    "\n",
    "        # backward \n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta.sum(1).reshape([len(delta), 1]) # reshape to (n x 1) matrix\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta.sum(1).reshape([len(delta), 1]) \n",
    "            nabla_w[-l] = np.dot(delta, activations[-l - 1].transpose())\n",
    "\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 9082 / 10000\n",
      "Epoch 1 : 9228 / 10000\n",
      "Epoch 2 : 9303 / 10000\n",
      "Epoch 3 : 9353 / 10000\n",
      "Epoch 4 : 9346 / 10000\n",
      "Epoch 5 : 9414 / 10000\n",
      "Epoch 6 : 9429 / 10000\n",
      "Epoch 7 : 9423 / 10000\n",
      "Epoch 8 : 9422 / 10000\n",
      "Epoch 9 : 9459 / 10000\n",
      "Epoch 10 : 9452 / 10000\n",
      "Epoch 11 : 9460 / 10000\n",
      "Epoch 12 : 9465 / 10000\n",
      "Epoch 13 : 9479 / 10000\n",
      "Epoch 14 : 9477 / 10000\n",
      "Epoch 15 : 9474 / 10000\n",
      "Epoch 16 : 9457 / 10000\n",
      "Epoch 17 : 9474 / 10000\n",
      "Epoch 18 : 9454 / 10000\n",
      "Epoch 19 : 9489 / 10000\n",
      "Epoch 20 : 9493 / 10000\n",
      "Epoch 21 : 9484 / 10000\n",
      "Epoch 22 : 9491 / 10000\n",
      "Epoch 23 : 9495 / 10000\n",
      "Epoch 24 : 9485 / 10000\n",
      "Epoch 25 : 9493 / 10000\n",
      "Epoch 26 : 9502 / 10000\n",
      "Epoch 27 : 9474 / 10000\n",
      "Epoch 28 : 9491 / 10000\n",
      "Epoch 29 : 9498 / 10000\n",
      "--- 52.02427625656128 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "training_data, validation_data, test_data = load_data_wrapper()\n",
    "net_matrix = NetworkMatrix([784, 30, 10])\n",
    "net_matrix.SGD(training_data, 30, 10, 3.0, test_data=test_data)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time take was only 52 secs with the min batch size =10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing both the  results, program with a mini-batch size of 10, the learning is a bit faster that when the size is equal to 1, but model will have low accuracy. The reason behind this behavior is that, on the first trial, the weights are updated much more often than on the second trial, since they are updated right after processing each example, while on the second trial, the weights aren’t updated until all the 10 examples are processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
